import torch

# 2.1 数据操作
# 2.1.1、基本操作与运算
# 2.1.1.1、arange 创建一个行向量 x，行向量包含以0开始的前12个整数，打印结果tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
x = torch.arange(12)
print("原始张量：", x)
# 2.1.1.2、张量的shape属性来访问张量（沿每个轴的长度）的形状
x_shape = x.shape
print("原始张量形状：", x_shape)
# 2.1.1.3、张量中元素的总数
x_size = x.numel()
print("原始张量元素总数：", x_size)
# 2.1.1.4、改变一个张量的形状，张量的元素值、大小并没有变，打印结果
# tensor([[ 0,  1,  2,  3],
#         [ 4,  5,  6,  7],
#         [ 8,  9, 10, 11]])
y = x.reshape(3, 4)
print("转变三行四列后张量：", y, "转换后张量形状：", x.shape,"转换后张量元素总数：", x.numel())
# 2.1.1.5、全零张量，打印结果
# tensor([[[0., 0., 0., 0.],
#          [0., 0., 0., 0.],
#          [0., 0., 0., 0.]],
#         [[0., 0., 0., 0.],
#          [0., 0., 0., 0.],
#          [0., 0., 0., 0.]]])
x_zero = torch.zeros((2, 3, 4))
print("全零张量：", x_zero)
# 2.1.1.6、全一张量，打印结果
#tensor([[[1., 1., 1., 1.],
#         [1., 1., 1., 1.],
#         [1., 1., 1., 1.]],
#       [[1., 1., 1., 1.],
#        [1., 1., 1., 1.],
#         [1., 1., 1., 1.]]])        
x_one = torch.ones((2, 3, 4))
print("全一张量：", x_one)
# 2.1.1.7、从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样，打印结果
# tensor([[-0.0135,  0.0665,  0.0912,  0.3212],
#         [ 1.4653,  0.1843, -1.6995, -0.3036],
#         [ 1.7646,  1.0450,  0.2457, -0.7732]])
x_randn = torch.randn(3, 4)
print("随机张量：", x_randn)
# 2.1.1.8、通过提供包含数值的Python列表，为所需张量中的每个元素赋予确定值，打印结果
# tensor([[2, 1, 4, 3],
#         [1, 2, 3, 4],
#         [4, 3, 2, 1]])
x_arr = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
print("通过提供包含数值的Python列表张量：", x_arr)
# 2.1.1.9、向量的加减乘除，打印结果
# (tensor([ 3.,  4.,  6., 10.]),
#  tensor([-1.,  0.,  2.,  6.]),
#  tensor([ 2.,  4.,  8., 16.]),
#  tensor([0.5000, 1.0000, 2.0000, 4.0000]),
#  tensor([ 1.,  4., 16., 64.]))
x = torch.tensor([1.0, 2, 4, 8])
y = torch.tensor([2, 2, 2, 2])
print("加法：", x + y,"，减法：", x - y,"，乘法：", x * y,"除法：", x / y,"，求幂：", x ** y)  
# 2.1.1.10、向量按元素指数运算，打印结果
# tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])
x_exp = torch.exp(x)
print("指数运算：", x_exp)
# 2.1.1.11、连结两个张量，打印结果
# (tensor([[ 0.,  1.,  2.,  3.],
#          [ 4.,  5.,  6.,  7.],
#          [ 8.,  9., 10., 11.],
#          [ 2.,  1.,  4.,  3.],
#          [ 1.,  2.,  3.,  4.],
#          [ 4.,  3.,  2.,  1.]]),
#  tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
#          [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
#          [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))
tmp1 = torch.arange(12, dtype=torch.float32).reshape((3,4))
tmp2 = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
print("按行连结：", torch.cat((tmp1, tmp2), dim=0), "按列连结：", torch.cat((tmp1, tmp2), dim=1))   
# 2.1.1.12、求和，打印结果
# tensor(66.)
print("求和：", tmp1.sum())

# 2.1.2、广播机制通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；
# 对生成的数组执行按元素操作。通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；
# 对生成的数组执行按元素操作。
# 2.1.2.1、不同形状可以使用广播机制进行操作
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
# 打印结果为
# (tensor([[0],
#          [1],
#          [2]]),
#  tensor([[0, 1]]))
print("三行一列张量：", a , "一行两列张量：", b)
# 相加，a将复制列， b将复制行，然后再按元素相加。打印结果为
# tensor([[0, 1],
#         [1, 2],
#         [2, 3]])
print("两个不同现状张量相加：", a + b)

# 2.1.3、索引和切片，就像在任何其他Python数组中一样，张量中的元素可以通过索引访问。 
# 与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是-1； 
# 可以指定范围以包含第一个元素和最后一个之前的元素。
# 2.1.3.1、可以用[-1]选择最后一个元素，可以用[1:3]选择第二个和第三个元素，打印结果
# (tensor([ 8.,  9., 10., 11.]),
#  tensor([[ 4.,  5.,  6.,  7.],
#          [ 8.,  9., 10., 11.]]))
tmp3 = torch.arange(12, dtype=torch.float32).reshape((3,4))
print("原始张量：", tmp3, "最后一个元素：", tmp3[-1], "第二个和第三个元素：", tmp3[1:3])
# 2.1.3.2、指定索引来将元素写入，打印结果
# tensor([[ 0.,  1.,  2.,  3.],
#         [ 4.,  5.,  9.,  7.],
#         [ 8.,  9., 10., 11.]])
tmp3[1, 2] = 9
print("更改第二行第三列的值为9后的张量：", tmp3)
# 2.1.3.3、为多个元素赋值相同的值，
# 只需要索引所有元素，然后为它们赋值。 
# 例如，[0:2, :]访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素
# 打印结果
# tensor([[12., 12., 12., 12.],
#         [12., 12., 12., 12.],
#         [ 8.,  9., 10., 11.]])
tmp3[0:2, :] = 12
print("第一行第二行所有列更改为12后的张量", tmp3)

# 2.1.4、节省内存的操作
# 原始操作
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
before = id(Y)
Y = Y + X
# 下面这行结果是false，因为Python首先计算Y + X，为结果分配新的内存，然后使Y指向内存中的这个新位置。
id(Y) == before
# 这可能是不可取的，原因有两个：
# 首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；
# 如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。
# 执行原地操作非常简单。 我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如Y[:] = <expression>。
# 为了说明这一点，我们首先创建一个新的矩阵Z，其形状与另一个Y相同， 使用zeros_like来分配一个全的块。
Z = torch.zeros_like(Y)
# 两个id(Z)值应该是一样的
print('id(Z):', id(Z))
Z[:] = X + Y
print('id(Z):', id(Z))
# 如果在后续计算中没有重复使用X， 我们也可以使用X[:] = X + Y或X += Y来减少操作的内存开销。
before = id(X)
X += Y
# 下面应该是true
id(X) == before

# 2.1.5、转换为其他Python对象
A = X.numpy()
B = torch.tensor(A)
# 下面两个的输出结果(numpy.ndarray, torch.Tensor)
type(A), type(B)
# 要将大小为1的张量转换为Python标量，我们可以调用item函数或Python的内置函数。
a = torch.tensor([3.5])
# 下面的输出结果(tensor([3.5000]), 3.5, 3.5, 3)
a, a.item(), float(a), int(a)

# 2.1.6、总结
# 深度学习存储和操作数据的主要接口是张量（维数组）。
# 它提供了各种功能，包括基本数学运算、广播、索引、切片、内存节省和转换其他Python对象。
